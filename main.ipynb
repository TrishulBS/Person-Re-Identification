{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "gdown https://drive.google.com/uc?id=0B8-rUzbwVRk0c054eEozWG9COHM\n",
    "unzip -qq Market-1501-v15.09.15.zip\n",
    "mv /content/Market-1501-v15.09.15 /content/Market-1501\n",
    "pip install wandb faiss-gpu\n",
    "wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MARKET_DATA_DIR = '/root/Market-1501/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Market1501Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.jpg')])\n",
    "        \n",
    "        self.label_to_images = {}\n",
    "        for image_file in self.image_files:\n",
    "            label = int(image_file.split('_')[0])\n",
    "            if label not in self.label_to_images:\n",
    "                self.label_to_images[label] = []\n",
    "            self.label_to_images[label].append(image_file)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor_image_file = self.image_files[idx]\n",
    "        anchor_label = int(anchor_image_file.split('_')[0])\n",
    "        \n",
    "        positive_image_file = np.random.choice([img for img in self.label_to_images[anchor_label] if img != anchor_image_file])\n",
    "        \n",
    "        negative_label = np.random.choice([label for label in self.label_to_images.keys() if label != anchor_label and label != -1 and label != 0])\n",
    "        negative_image_file = np.random.choice(self.label_to_images[negative_label])\n",
    "        \n",
    "        anchor_img = self.load_image(anchor_image_file)\n",
    "        positive_img = self.load_image(positive_image_file)\n",
    "        negative_img = self.load_image(negative_image_file)\n",
    "        \n",
    "        return (anchor_img, positive_img, negative_img), anchor_label\n",
    "    \n",
    "    def load_image(self, image_file):\n",
    "        img_name = os.path.join(self.root_dir, image_file)\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = Market1501Dataset(os.path.join(MARKET_DATA_DIR, 'bounding_box_train'), transform=transform)\n",
    "test_dataset = Market1501Dataset(os.path.join(MARKET_DATA_DIR, 'bounding_box_test'), transform=transform)\n",
    "query_dataset = Market1501Dataset(os.path.join(MARKET_DATA_DIR, 'query'), transform=transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "query_loader = DataLoader(query_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def unnormalize(img):\n",
    "#     mean = np.array([0.485, 0.456, 0.406])[:, None, None]\n",
    "#     std = np.array([0.229, 0.224, 0.225])[:, None, None]\n",
    "#     img = std * img + mean\n",
    "#     img = np.clip(img, 0, 1)\n",
    "#     return img\n",
    "\n",
    "# Function to display an image\n",
    "def imshow(img):\n",
    "    img = img.numpy()\n",
    "    img = np.transpose(img, (1, 2, 0)) # change this line\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "(anchor_img, positive_img, negative_img), anchor_label = train_dataset[0]\n",
    "print(f\"Anchor label: {anchor_label}\")\n",
    "imshow(torchvision.utils.make_grid([anchor_img, positive_img, negative_img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(anchor_img, positive_img, negative_img), anchor_label = test_dataset[-1]\n",
    "print(f\"Identity: {anchor_label}\")\n",
    "imshow(torchvision.utils.make_grid([anchor_img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save_model = 'model'\n",
    "Path(path_save_model).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "model = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "embedding_size = 128\n",
    "model.fc = nn.Linear(model.fc.in_features, embedding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin=0.02)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = []\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_total = 0\n",
    "        for i, (triplets, labels) in tqdm(enumerate(train_loader)):\n",
    "     \n",
    "            anchor, positive, negative = [img.to(device) for img in triplets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "           \n",
    "            anchor_embeddings = model(anchor)\n",
    "            positive_embeddings = model(positive)\n",
    "            negative_embeddings = model(negative)\n",
    "\n",
    "\n",
    "            anchor_embeddings = nn.functional.normalize(anchor_embeddings, p=2, dim=1)\n",
    "            positive_embeddings = nn.functional.normalize(positive_embeddings, p=2, dim=1)\n",
    "            negative_embeddings = nn.functional.normalize(negative_embeddings, p=2, dim=1)\n",
    "            \n",
    "            \n",
    "            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_total += loss.item()\n",
    "            wandb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"lr\": optimizer.param_groups[0]['lr'],\n",
    "            })\n",
    "\n",
    "        loss_batch = loss_total / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss_batch}')\n",
    "        wandb.log({\"loss_batch\": loss_batch})\n",
    "        loss_all.append(loss_batch)\n",
    "      \n",
    "        torch.save(model, os.path.join(path_save_model, 'model_latest.pth'))\n",
    "        if loss_batch == min(loss_all):\n",
    "            torch.save(model, os.path.join(\n",
    "                path_save_model, 'model_best_loss.pth'))\n",
    "\n",
    "wandb.init(project=\"human-recognition\")\n",
    "train(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
